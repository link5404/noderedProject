<script type="text/javascript">
    RED.nodes.registerType('spark-filter', {
        category: 'spark',
        color: '#E6E0F8',
        defaults: {
            name: { value: "" },
            filterExpression: { value: "", required: true },
            filterTable: { value: "", required: true }
        },
        inputs: 1,
        outputs: 1,
        icon: "filter.png",
        label: function() {
            return this.name || "spark-filter";
        },
        paletteLabel: "filter",
        oneditprepare: function() {
            // Optional: Add any initialization code for your edit dialog here
        }
    });
</script>

<script type="text/html" data-template-name="spark-filter">
    <div class="form-row">
        <label for="node-input-filterTable"><i class="fa fa-table"></i> Table Name</label>
        <input type="text" id="node-input-filterTable" placeholder="e.g: airports_data.csv">
    </div>
    <div class="form-row">
        <label for="node-input-filterExpression"><i class="fa fa-code"></i> Filter Expression</label>
        <input type="text" id="node-input-filterExpression" placeholder="col > 10">
    </div>
    <div class="form-tips">Enter a filter expression (e.g., "col > 10" or "col.contains('text'), or use a lambda function. lambda x: x[0] > 10")</div>
</script>

<script type="text/html" data-help-name="spark-filter">
    <p>A node that adds a filter operation to Spark operations JSON.</p>
    <p>This node appends a filter method to the "Methods" array in the message payload.</p>
    <h3>Inputs</h3>
    <dl class="message-properties">
        <dt>payload<span class="property-type">object</span></dt>
        <dd>The JSON object containing a "Methods" array to which the filter operation will be added.</dd>
    </dl>
    <h3>Outputs</h3>
    <dl class="message-properties">
        <dt>payload<span class="property-type">object</span></dt>
        <dd>The modified JSON object with the filter operation added to the "Methods" array.</dd>
    </dl>
    <h3>Details</h3>
    <p>Use this node to add filtering criteria to your Spark operations. The filter expression should be a valid condition that will be translated to a Spark filter operation.</p>
    <p>Specify the table name on which the filter should be applied. This should match one of the table names in your available Tables.</p>
</script>